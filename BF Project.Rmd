---
title: "BF Project"
author: "Madhuri Dilliker"
date: "26 November 2019"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
#install.packages("ggplot2")
library(ggplot2) 
library(fpp2) 
library(forecast)
library(fma)
library(expsmooth)    
library(quantmod) #install.packages("quantmod") 
#library(plotly)
#install.packages("ggfortify")
#library(ggfortify)
library(tseries)
library(gridExtra)
#install.packages("docstring")
#library(docstring)
library(readr)
```

```{r}
Sys.Date()
getSymbols("AMZN",from = as.Date("2008-01-04"), to = as.Date("2019-10-01"))
amzn<-data.frame(to.monthly(AMZN))
amzn<-c(to.monthly(AMZN))
```

#Cleaning DATASET 
Check for any missing data
```{r}
sum(is.na(amzn))
summary(amzn)
```

##Exploratory Analysis
Now we want to get a feel for our data to get an intuition about the models that may be appropriate for our forecast. For this, we plot our data and diagnose for trend, seasonality, heteroskedasticity, and stationarity.


#Creating Time-Series data object
We are considering the closing value for stock price. Fetching this column and creating a monthly time series..

```{r}
amzn_close = round(amzn$AMZN.Close,2)
amzn_close <- ts(data=amzn_close,start=c(2008, 1), freq=12)
```

Before we begin any analysis, we will be splitting the data to remove 2018 & 2019 to use as our test set.

```{r}
#Training set
amzn_close_train<-ts(data=amzn_close,start=c(2008, 1), end=c(2017,12), freq=12)
#Test set
amzn_close_test<-ts(data=amzn_close,start=c(2018, 1), end=c(2019,09), freq=12)
```

#Plotting our Time series
Plotting the data is arguably the most critical step in the exploratory analysis phase..(We chose to emphasize on the time series object that has intervals from 2008 to 2017). This enables us to make inferences about important components of the time-series data, such as trend, seasonality, heteroskedasticity, and stationarity. 
Here is a quick summary of each:

Trend: We say that a dataset has a trend when it has either a long-term increase or decrease.
Seasonality: We say that a dataset has seasonality when it has patterns that repeat over known, fixed periods of time (e.g. monthly, quarterly, yearly).
Heteroskedasticity: We say that a data is heteroskedastic when its variability is not constant (i.e. its variance increases or decreases as a function of the explanatory variable).
Stationarity: A stochastic process is called stationary if the mean and variance are constant 
(i.e. their joint distribution does not change over time).

#Time plot to observe the trend, seasonality or cyclic pattern
```{r}
autoplot(amzn_close) + xlab("Year") + ylab("Closing Price of Amazon")+
  ggtitle("Timeseries plot of Amazon closing price")
```
The Amazon stock price shows a strong increasing trend with no significant seasonality. However, there is no evidence of cyclic behaviour..

#Decomposing Time Series
```{r}
fit<-stl(amzn_close,s.window = 5)
plot(fit)
```

Trend is showing positive growth. Let's plot the seasonal plot to plot the data against the individual "seasons" in which the data is observed.
```{r}
ggseasonplot(amzn_close, year.labels = TRUE, year.labels.left = TRUE) + ylab("Closing price") + 
  xlab(" Dates across year") + ggtitle("Seasonal Plot: Amazon Stock Prices")

ggseasonplot(amzn_close, polar=TRUE) + ylab("Closing price") + 
  xlab(" Dates across year") + ggtitle("Seasonal Plot: Amazon Stock Prices")
```
We do not see any pattern for any particular season.. We cannot observe any seasonality.
We observe sudden increase in the stock price from the end of year 2015.

#Let's explore the relationship more..
```{r}
qplot(amzn$AMZN.Close,amzn$AMZN.Open, data=amzn)
#They have a linear relationship
qplot(amzn$AMZN.Volume,amzn$AMZN.Close, data=amzn)
#here, the volume is high only when the closing price is low..
```

```{r}
a<-to.monthly(AMZN)
#Let's plot both the time series..
autoplot(a[,c("AMZN.Close","AMZN.Volume")], facets=TRUE) +
  xlab("Years") + ylab("") +
  ggtitle("CLosing Price & Volume")
```

Here, we can see an inverse relation between closing price ad volumne of the Stock.

#Scatterplot Matrix
```{r}
library(GGally)
ggpairs(amzn[,1:5])
```
We can see that the variables are highly correlated and the relationship is also linear.. VOlume is not highly correlated.

Show the trend cycle component..No seasonality.Only Trend
```{r}
plot(amzn_close, col="gray",
     main="Amazon stock price",
     ylab="Date", xlab="")
lines(fit$time.series[,2],col="red",ylab="Trend")
```

For the seasonally adjusted data---seasonality is removed
```{r}
plot(amzn_close, col="grey",
     main="Amazon Stock Market",
     xlab="", ylab="Date")
lines(seasadj(fit),col="red",ylab="Seasonally adjusted")
```
We observe non-seasonality in the dataset...

Let's plot the Lagged scatterplot
```{r}
gglagplot(amzn_close)
```
Here the color indicate the month of the variable on the vertical axis.

#ACF
```{r}
ggAcf(amzn_close)
ggPacf(amzn_close)
```

Here we see a strongly increasing trend in the data and all lags are significant..

They are highly autorelation at initial lags but tend to decrease gradually.. r1 >r2>r3>r4>r5.....
When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase.
We see no seasonality/cyclic behaviour in the correlogram...
ACF stands for "autocorrelation function". 
The ACF diagnosis is employed over a time-series to determine the order for which we are going to create our model using ARIMA modeling. Loosely speaking, a time-series is stationary when its mean, variance, and autocorrelation remain constant over time. These functions help us understand the correlation component of different data points at different time lags. Lag refers to the time difference between one observation and a previous observation in a dataset.

When there is large autocorrelation within our lagged values, we see geometric decay in our plots, which is a huge indicator that we will have to take the difference of our time series object.

#Check for Seasonality?
Here, we are using KPSS test. In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false..
```{r}
library(urca)
Test=ur.kpss(amzn_close)
summary(Test)
```
We observe that the value of test-statistic is greater than the critucal value at confidence value=0.95. Hence, we reject the null Hypothesis and say that our data is non stationary..

We will most likely have to difference our time series for stationarity. Let's see how many difference we need?
```{r}
ndiffs(amzn_close)
```
Transforming our data to adjust for non-stationary:
From visual inspection of the time-series object and the other graphs used for exploratory purposes we decided it is appropriate to difference our time series object to account for the non-stationarity and see how that fares!

A way to make a time-series stationary is to find the difference across its consecutive values. This helps stabilize the mean, thereby making the time-series object stationary.

Transforming and Testing for Seasonality..
```{r}
amzn_diff<-diff(amzn_close)
Test=ur.kpss(amzn_diff)
summary(Test)
```
Now the data has been transformed into season..

#Plotting the transformed data..
```{r}
autoplot(amzn_diff) + xlab("Year") + ylab("Closing Price of Amazon")+
  ggtitle("Timeseries plot of Amazon closing price")
```
This plot suggests that our working data is stationary. We want to confirm this running an ACF diagnostics over this data to find our if we can proceed to estimating a model.

#ACF
```{r}
ggAcf(amzn_diff)
```
#adf test
```{r}
adf.test(amzn_close)

```
H0: Data is stationary or not
H1:Data is Stationary
p value for the ADF test is relatively high. For that reason, we need to do some further visual inspection - but we know we will most likely have to difference our time series for stationarity.

#autoregressive models
In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself.

#for AR(1) model
```{r}
AR_fit<-Arima(amzn_close, order=c(1,0,0))
AR_fit
forecast_ar_model<-forecast(AR_fit,h=10)
forecast_ar_model
plot(amzn_close,col='red')
lines(fitted(AR_fit),col='grey')
plot(forecast_ar_model,col="blue")
lines(fitted(AR_fit),col="red")
lines(forecast_ar_model$mean,col="black")


```
#Moving Average models
a moving average model uses past forecast errors in a regression-like model.
#for MA(1)

```{r}
MA_fit<-Arima(amzn_close_train, order=c(0,0,2))
MA_fit
forecast_ma_model<-forecast(MA_fit,h=10)
forecast_ma_model
summary(MA_fit)
accuracy(forecast_ma_model,amzn_close_test)




plot(forecast_ma_model,col="blue")
lines(fitted(MA_fit),col="red")
lines(forecast_ma_model$mean,col="black")


```
#ARMA Model
If we combine differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. 
#with AR(1),diff =1, MA(2)
```{r}
fit_arma<-Arima(amzn_close_train, order=c(1,1,2))
fit_arma
forecast_arma<-forecast(fit_arma,h=5)
forecast_arma
summary(fit_arma)
accuracy(forecast_arma,amzn_close_test)
plot(forecast_arma,col="blue")
lines(fitted(fit_arma),col="red")
lines(forecast_arma$mean,col="black")


```

#best ARIMA Model using auto.arima
The auto.arima() function in R uses a variation of the Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008), which combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model. 
gives the closest best model, output is stationary data with n diff (ndiff is difference needed to obtain the statinary data)
has min AIC after differecing 
```{r}
fit<-auto.arima(amzn_close_train,seasonal = FALSE)
fit
fitted(fit)
fit2=forecast(fit,h=12)
summary(fit)
accuracy(fit2,amzn_close_test)
plot(fit2,col="blue")
lines(fitted(fit),col="red")
lines(fit2$mean,col="black")
checkresiduals(fit)

```
```{r}
```
Acf -> Relation of previous correlation lag
Pacf-> Partial auto correalation 
Finding p and q values using ACF and PACF
AR(p)
1) Check ACF 
if decreasing go to step 2
2) Check Pacf
here check for significant number


MA(q)
1) Check PACF 
if decreasing go to step 2
2) Check ACF
here check for last significant number

```{r}
ggAcf(amzn_close)
ggPacf(amzn_close)
```

Forecasting Methods
1) Mean
```{r}


mean_method_fit<-meanf(amzn_close_train,h=12)
autoplot(window(amzn_close,start=2008))+autolayer(mean_method_fit,series="meanf",PI=FALSE)+xlab("Time")+ylab("Closing")+
  ggtitle("Amazon forecast using mean")+guides(colour=guide_legend(title = "Forecast"))
accuracy(mean_method_fit,amzn_close_test)


```

2) snaive

```{r}

snaive_fit<-snaive(amzn_close_train,h=12)
autoplot(window(amzn_close,start=2008))+autolayer(snaive_fit,series="Snaive",PI=FALSE)+xlab("Time")+ylab("sales")+
ggtitle("Amazon forecast using snaive")+guides(colour=guide_legend(title = "Forecast"))
accuracy(snaive_fit,amzn_close_test)
```
3) Naive
```{r}
naive_fit<-rwf(amzn_close_train,h=12)
autoplot(window(amzn_close,start=2008))+autolayer(snaive_fit,series="naive",PI=FALSE)+xlab("Time")+ylab("sales")+
ggtitle("Amazon forecast using naive")+guides(colour=guide_legend(title = "Forecast"))
accuracy(naive_fit,amzn_close_test)



```

